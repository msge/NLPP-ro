{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c65472a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing neccessary packages\n",
    "import string#in built class string for string related operations\n",
    "import sys#in  built class sys to check system arguments given\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import glob\n",
    "import codecs\n",
    "import math\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f2d721",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a444adac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "የሚፈልጉትን ጥያቄ ካስገቡ ቡኋላ እባከዎ ENTER ይበሉና ጥቂት ደቂቃ ይታገሱ ！ከዘገይበዎት በደጋሚ ENTER ይበሉ :-\n",
      "የሚፈልጉትን ጥያቄ ያስገቡየባህርዳር ዩኒቨርሲቲ ተማሪ\n",
      "**********************************************************************\n",
      "የተገኙት መዛግብት ባላቸው ቅርበት መሰረት እንደሚክተለው ተቀምጠዋል። \n",
      "**********************************************************************\n",
      "ደረጃ\tየመረጃ ስም \tየመመሳሰል ቅርበት\n",
      "---------   ---------------       -----------------------------\n",
      "1 \t Doc15.txt \t\t 0.2343622856774771\n",
      "2 \t Doc17.txt \t\t 0.21723923643753654\n",
      "3 \t Doc8.txt \t\t 0.20005374784437183\n",
      "4 \t Doc12.txt \t\t 0.16714377865112756\n",
      "5 \t Doc11.txt \t\t 0.1415564057871132\n",
      "6 \t Doc2.txt \t\t 0.11210966198842147\n",
      "7 \t Doc4.txt \t\t 0.10510637953326624\n",
      "8 \t Doc7.txt \t\t 0.08394918091520517\n",
      "9 \t Doc1.txt \t\t 0.07577179005512108\n",
      "10 \t Doc9.txt \t\t 0.05941820509070592\n",
      "______________________________________________\n",
      "   ማንበብ ይፈልጋሉ ?  የመረጃዉን ስም ተጠቅምዉ ይምረጡ ።ያለበለዚያ ሌላ ፊደል ይጫኑDoc15.txt\n",
      "መረጃዉ እንደሚከተለዉይ ንበባል\n",
      "_____________________________________________________________________________\n",
      "﻿ባህርዳር ዩኒቨርሲቲ አምስተኛውን የትምህርት ጥራት ኮንፈረንስ አካሄደ::\n",
      "   እንደግና ፍለጋ ለመሞከር ይፈልጋሉ? ከፈለጉ Y ቁጥርን ይጫኑ፤ ያለበለዚያ ሌላ ፊደል ይጫኑ።\n",
      "                           \n",
      "\t እናመሰግናለን!  እንደገና ይጎብኙን!\n"
     ]
    }
   ],
   "source": [
    "print(\"የሚፈልጉትን ጥያቄ ካስገቡ ቡኋላ እባከዎ ENTER ይበሉና ጥቂት ደቂቃ ይታገሱ ！ከዘገይበዎት በደጋሚ ENTER ይበሉ :-\")\n",
    "\n",
    "class Amharic_search():\n",
    "\n",
    "    def indexer(path): \n",
    "        def pucrmv(str):#defining a function that helps to remove punctuatuion\n",
    "            str=re.sub('[.''፡፡''!፡()\"፣?/!።]','',str)#regular expration is subdivided in to sub string\n",
    "            if str.endswith('\\n'):#check wheather the given string is ends with  new line character or not\n",
    "                str=str.rstrip('\\n')#removes  \\n characters from right\n",
    "            if str.startswith(' '):#check is the given string is starts with space\n",
    "                str=str.lstrip(' ')#removes space characters frrom the left\n",
    "            s=codecs.open(\"Am_pun.txt\",'r', encoding = 'utf-8' )#Open an encoded file using the given mode and \n",
    "            #codecs return an instance of StreamReaderWriter , providing transparent encoding/decoding.\n",
    "            puc = s.read()#read punctuations from the text file\n",
    "            for pc in puc:#iterate through all the marks\n",
    "                if str.endswith(pc):#chcke is the given string ends with pppunctuation\n",
    "                    str=str.rstrip(pc)#remove it\n",
    "                if str.startswith(pc):#chcke is the given string start with pppunctuation\n",
    "                    str=str.lstrip(pc)#remove it\n",
    "            return str#return the string after removing punctuation marks\n",
    "        def normal(str):#defining function for normalization; which means treating leters that has the same phonems in simiar \n",
    "\n",
    "            let=codecs.open(\"Am_nor.txt\",'r', encoding = 'utf-8' )#open text file that has normals\n",
    "            nor = let.read()#read each letters\n",
    "            li=[]#define a list that used to contain hohies that useful for normalization\n",
    "            for j in nor.split():\n",
    "                li.append(j)\n",
    "                \n",
    "            for i in range(0,len(li)-1,2):\n",
    "                str=str.replace(li[i],li[i+1])\n",
    "            return str\n",
    "\n",
    "        def stem(word):#hfunction for finding the stem word; which means by removing the prefix orsuffix\n",
    "            s=codecs.open(\"Am_suf.txt\",'r', encoding = 'utf-8' )\n",
    "            su = s.read()\n",
    "            p=codecs.open(\"Am_pre.txt\",'r', encoding = 'utf-8' )\n",
    "            pr = p.read()#read the prefix list from text file\n",
    "\n",
    "            for suf in su.split():#loop or iterate through the suffix term list\n",
    "                if len(word)>2:#check the word length \n",
    "                    \n",
    "                    if word.endswith(suf):#check the word is ends with suffix list or not\n",
    "                           word=word [ :-len(suf) ]#remove the suffix from the word and re assign the word\n",
    "            for pre in pr.split():\n",
    "                if len(word)>2:\n",
    "                    if word.startswith(pre):\n",
    "                        l=len(pre)\n",
    "                        word=word[l:]\n",
    "            word=normal(word)\n",
    "            return word\n",
    "        def readfile(filename):#for reading the file\n",
    "            indexterms=[]\n",
    "            infile=codecs.open(filename, encoding='utf-8')\n",
    "            lterms=infile.readlines()\n",
    "            infile.close()\n",
    "            for str in lterms:\n",
    "                if str.endswith('\\n'):\n",
    "                    str=str.rstrip('\\n')\n",
    "                indexterms.append(str)\n",
    "            return indexterms \n",
    "        def doclister(pst):\n",
    "            ldocid=[]\n",
    "            infile=open(pst)\n",
    "            lterms=infile.readlines()\n",
    "            infile.close()\n",
    "            for str in lterms:\n",
    "                if str.endswith('\\n'):\n",
    "                    str=str.rstrip('\\n')\n",
    "                ldocid.append(str)\n",
    "            return ldocid\n",
    "        def iterm(ldocid):\n",
    "            fileterms=[]\n",
    "            \n",
    "            indexterms=[]\n",
    "            indexterm=[]\n",
    "            stopw=readfile('Am_sto.txt')\n",
    "            for docid in ldocid:\n",
    "                fileterms=readfile(docid)\n",
    "                for listterm in fileterms:\n",
    "                    for index in listterm.split():\n",
    "                        flag=0\n",
    "                        for i in range(0,len(stopw)):\n",
    "                            if stem(pucrmv(index))in stopw[i]:\n",
    "                                flag=1\n",
    "                        if flag==0:             \n",
    "                            indexterms.append(stem(pucrmv(index)))\n",
    "            for it in range(0,len(indexterms)):\n",
    "                flag=0\n",
    "                for jj in range(0,len(indexterm)):\n",
    "                    if indexterms[it] == indexterm[jj]:\n",
    "                        flag=1\n",
    "                if flag==0:\n",
    "                    indexterm.append(indexterms[it])\n",
    "                    #print(\"term=\",indexterm)\n",
    "            #for i in range(0,len(indexterm)):\n",
    "                #print (indexterm[i])\n",
    "            return indexterm\n",
    "    \n",
    "        def termfreq(indexterm,ldocid):\n",
    "            f=0\n",
    "            cf=0\n",
    "            tfiles=[]\n",
    "            var=0\n",
    "            dicindex=indexterm\n",
    "            for index in indexterm:\n",
    "                dicindex[var]={}\n",
    "                for doc in ldocid:\n",
    "                    tfiles=readfile(doc)\n",
    "                    f=0\n",
    "                    for str in tfiles: \n",
    "                        for term in str.split(\" \"):\n",
    "                            if stem(pucrmv(term))== index:\n",
    "                                f=f+1                                \n",
    "                    dicindex[var][doc]=f#stores the frequncy of each index term in each documnet in dicindex dictionary\n",
    "                var=var+1# increments index of each index term\n",
    "            \n",
    "            return dicindex\n",
    "        def userquery(indexlist, dicindex, ldocid):#the user query accepts those variable and return\n",
    "            \n",
    "            lquery=[]\n",
    "            iquery=[]\n",
    "            dicquery={}\n",
    "            stopw=readfile('Am_sto.txt')\n",
    "            query=input(\"የሚፈልጉትን ጥያቄ ያስገቡ\")\n",
    "            query=query.lstrip()\n",
    "            query=query.rstrip()\n",
    "            qlength=len(query.split(\" \"))\n",
    "            if len(query)>0:\n",
    "                if len(query)>=1:\n",
    "                    if qlength<15:\n",
    "                        # To remove stopword from the user query, to stem user query word and append the user query word in a list called lquery\n",
    "                        for qin in query.split(\" \"):\n",
    "                            flag=0\n",
    "                            for i in range(0,len(stopw)):\n",
    "                                if stem(pucrmv(qin))in stopw[i]:\n",
    "                                    flag=1\n",
    "                            if flag==0:\n",
    "                                lquery.append(stem(pucrmv(qin)))\n",
    "                                \n",
    "                        # To identify unique word from the user query and append them in a list called iquery\n",
    "                        idx=0\n",
    "                        for iq in lquery:\n",
    "                            if not iquery.__contains__(iq):\n",
    "                                if not iq =='':\n",
    "                                    iquery.append(iq)\n",
    "                            # To check the term whether it is in the corpus or not.\n",
    "                            #If it is not found then for error handling the term will be append in the indexterm,\n",
    "                            #and the dictionery(dicindex) that contain the stastical infromation about the term will be updated\n",
    "                            if not indexlist.__contains__(iq):\n",
    "                                indexlist.append(iq)\n",
    "                                idx=indexlist.index(iq)\n",
    "                                dicindex.append({})\n",
    "                                for doc in ldocid:\n",
    "                                    dicindex[idx][doc]=0\n",
    "\n",
    "                        # To callculate the term frequency of the user query and append them in a dictionary called dicquery\n",
    "                        for iq in iquery:\n",
    "                            f=0\n",
    "                            for term in lquery:\n",
    "                                if term == iq:\n",
    "                                    f=f+1                    \n",
    "                            dicquery[iq]=f\n",
    "\n",
    "                        #To identify the maximum term frequency of a document and append on a dictionary called maxfdoc\n",
    "                        #and this will be used to normalize the termfrequency\n",
    "                        maxfdoc={}\n",
    "                        ndoc=len(ldocid)\n",
    "                        for doc in ldocid:\n",
    "                            maxf=0\n",
    "                            for i in range(0,len(indexlist)):#changed \n",
    "                                if maxf<dicindex[i][doc]:\n",
    "                                    maxf=dicindex[i][doc]\n",
    "                            maxfdoc[doc]=maxf\n",
    "\n",
    "                        #TO calculate document frequency for each indexterm and append it in a dictionary called dicndci\n",
    "                        #it will be used to callculate inverse document frequency\n",
    "                        dicndci={}\n",
    "                        for iq in range(0,len(indexlist)):\n",
    "                            ndci=0\n",
    "                            tindex=indexlist.index(indexlist[iq])\n",
    "                            for ld in ldocid:\n",
    "                                if dicindex[tindex][ld]>0:\n",
    "                                    ndci=ndci+1\n",
    "                            dicndci[indexlist[iq]]=ndci\n",
    "\n",
    "                        #To calculate similarity between each documents and query using cosine similarity\n",
    "                        #the term weitght here is that it uses tfidf\n",
    "                        #and it append the document and its similarity value in the a dictionary called diccos\n",
    "                        diccos={}\n",
    "                        for doc in ldocid:\n",
    "                            upv=0\n",
    "                            lvq=0\n",
    "                            lvd=0\n",
    "                            lv=0\n",
    "                            value=0.0           \n",
    "                            for iq in indexlist:\n",
    "                                tindex=indexlist.index(iq)\n",
    "                                #to calculate the normalized term frequency for the indexterm iq\n",
    "                                tf=(dicindex[tindex][doc]+0.0)/maxfdoc[doc]\n",
    "                                if not dicndci[iq]==0:\n",
    "                                    #to calculate the inverse document frequency for the indexterm iq\n",
    "                                    idf=math.log(((len(ldocid)+0.0)/dicndci[iq]),2)\n",
    "                                else:\n",
    "                                    idf=0\n",
    "                                tfidf=tf*idf\n",
    "                                #if the indexterm is found in the query\n",
    "                                if iquery.__contains__(iq):\n",
    "                                    upv=upv+dicquery[iq]*tfidf\n",
    "                                    lvq=lvq+pow(dicquery[iq],2)\n",
    "                                lvd=lvd+pow(tfidf,2)\n",
    "                            lv=math.sqrt((lvq*lvd))\n",
    "                            if not lv ==0:\n",
    "                                value=upv/lv\n",
    "                                diccos[doc]=value\n",
    "\n",
    "                        ranklist=[]\n",
    "                        ranklist=list(diccos.keys())\n",
    "\n",
    "                        #to check whether there is a relevant document or not\n",
    "                        flag=0\n",
    "                        for i in range(0,len(ranklist)):\n",
    "                            if diccos[ranklist[i]] !=0:\n",
    "                                flag=1\n",
    "                        #to rank the similarity of document with the query in decreasing order\n",
    "                        readf=[]\n",
    "                        for i in range(len(ranklist)):\n",
    "                            for j in range(i+1,len(ranklist)):\n",
    "                                if diccos[ranklist[i]]<diccos[ranklist[j]]:\n",
    "                                    temp=ranklist[i]\n",
    "                                    ranklist[i]=ranklist[j]\n",
    "                                    ranklist[j]=temp\n",
    "                        if flag==1:\n",
    "                            print(\"**********************************************************************\")\n",
    "                            print (\"የተገኙት መዛግብት ባላቸው ቅርበት መሰረት እንደሚክተለው ተቀምጠዋል። \")\n",
    "                            print(\"**********************************************************************\")\n",
    "                            print (\"ደረጃ\\tየመረጃ ስም \\tየመመሳሰል ቅርበት\")\n",
    "                            print (\"---------   ---------------       -----------------------------\")\n",
    "                            for i in range(len(ranklist)):\n",
    "                                if diccos[ranklist[i]]>0:\n",
    "                                   print (i+1,\"\\t\",ranklist[i][7:],\"\\t\\t\",diccos[ranklist[i]])\n",
    "                                   readf.append(ranklist[i][7:])\n",
    "                            print (\"______________________________________________\")\n",
    "                            read=input(\"   ማንበብ ይፈልጋሉ ?  የመረጃዉን ስም ተጠቅምዉ ይምረጡ ።ያለበለዚያ ሌላ ፊደል ይጫኑ\")\n",
    "                            content=\"\"\n",
    "                            if read in readf:\n",
    "                                print(\"መረጃዉ እንደሚከተለዉይ ንበባል\")\n",
    "                                print (\"_____________________________________________________________________________\")\n",
    "                                infile=codecs.open('corpus/'+read,'r',encoding = 'utf-8')\n",
    "                                content=infile.read()\n",
    "                                \n",
    "                                print(content)\n",
    "                                readds=input(\"   እንደግና ፍለጋ ለመሞከር ይፈልጋሉ? ከፈለጉ Y ቁጥርን ይጫኑ፤ ያለበለዚያ ሌላ ፊደል ይጫኑ።\")\n",
    "                                if readds=='Y':\n",
    "                                      userquery(indexlist, dicindex, ldocid)\n",
    "                                else:\n",
    "                                      print (\"                           \")\n",
    "                                      print (\"\\t እናመሰግናለን!  እንደገና ይጎብኙን!\")   \n",
    "                            else:\n",
    "                                print(\"   ይቅርታ ያስገቡት መረጃ በዝርዝሩ ዉስጥ የለም ።\")\n",
    "                                print (\"_____________________________________________________________________________\")\n",
    "                                readd=input(\"   እንደገና ፍለጋ ለመሞከር ይፈልጋሉ? ከፈለጉ 1 ቁጥርን ይጫኑ፤ ያለበለዚያ ሌላ ፊደል ይጫኑ።\")\n",
    "                                if readd=='1':\n",
    "                                    userquery(indexlist, dicindex, ldocid)\n",
    "                                else:\n",
    "                                    print (\"                           \")\n",
    "                                    print (\"\\t እናመሰግናለን!  እንደገና ይጎብኙን!\")\n",
    "                        else:\n",
    "                            print (\"ይቅርታ! ከጠየቁት መጠይቅ ጋር የሚመሳሰል መረጃ አልተገኘም።\")\n",
    "                            print (\"_____________________________________________________________________________\")\n",
    "                            readd=input(\"   እንደግና ፍለጋ ለመሞከር ይፈልጋሉ? ከፈለጉ 1 ቁጥርን ይጫኑ፤ ያለበለዚያ ሌላ ፊደል ይጫኑ።\")\n",
    "                            if readd=='1':\n",
    "                                  userquery(indexlist, dicindex, ldocid)\n",
    "                            else:\n",
    "                                  print (\"                           \")\n",
    "                                  print (\"\\t እናመሰግናለን!  እንደገና ይጎብኙን!\")    \n",
    "                            \n",
    "                    else:\n",
    "                        print (\"እባክዎ ከ 15 ያነሱ ቃላትን ይጠቀሙ።\")\n",
    "                        userquery(indexlist, dicindex, ldocid)\n",
    "                else:\n",
    "                    print (\"ያስገቡት መጠይቅ ተፈላጊ የሆኑ መረጃዎችን ለማግኘት አያስችልም።\")\n",
    "                    userquery(indexlist, dicindex, ldocid)\n",
    "            else:\n",
    "               print( \"\\tምንም ዓይነት መጠይቅ አላስገቡም፤ እባክዎ እንደገና ይሞክሩ\\n\")\n",
    "               userquery(indexlist, dicindex, ldocid)\n",
    "               \n",
    "       \n",
    "        return userquery(iterm(doclister(path)), termfreq(iterm(doclister(path)),doclister(path)),doclister(path))\n",
    "\n",
    "   \n",
    "path=\"Document_identifier.txt\"\n",
    "Amharic_search.indexer(path)\n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3909c33e",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e47cec7f",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb10657",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db0f403",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
